---
title: "Class9"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Section 1
Import data from csv file:
```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
wisc.matrix <- as.matrix(wisc.df)
row.names(wisc.matrix) <- wisc.df$id
```

Let's make a new data matrix with just the numeric values of interest = get columns 3 to 32
```{r}
wisc.data <- as.matrix(wisc.df[,3:32]) #note that row 33 is full of NAs that might screw up further analyses
```



Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.
```{r}
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```

Q1. How many observations are in this dataset?
```{r}
dim(wisc.matrix)
```


Q2. How many variables/features in the data are suffixed with _mean?
```{r}
length(grep("_mean", colnames(wisc.data)))
```

Q3. How many of the observations have a malignant diagnosis?
```{r}
sum(diagnosis)
```


#Section 2

The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.
Two common reasons for scaling data include: different units and/or significantly different variances.

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the  colMeans() and apply() functions like you’ve done before.

# Check column means and standard deviations
```{r}
colMeans(wisc.data)
plot(colMeans(wisc.data), type = "o")
```

```{r}
apply(wisc.data, #apply(x, MARGIN, FUN)
      2,  #where MARGIN = 1 if rows and =2 if columns
      sd)   #function applied
plot(apply(wisc.data, 2, sd), type = "o")
```


Since the standard deviations vary from 10^-3 to 10^+2, I will apply scaling
```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
plot(wisc.pr)
```

```{r}
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
```{r}
pca.var <- wisc.pr$sdev^2 ## Variance captured per PC
pca.var.per <- round(pca.var/sum(pca.var), 4) #percent value, rounded to make it look good
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```


Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
```{r}
sum(pca.var.per[1:2])
sum(pca.var.per[1:3])
sum(pca.var.per[1:6])
sum(pca.var.per[1:7]) # also see cummulative sum 'cumsum()' in the next section
```


##Interpreting PCA results
A common visualization for PCA results is the so-called biplot (rownames are used as the plotting character)
```{r}
biplot(wisc.pr)
```


Lets generate a more standard scatter plot of each observation along PC1 and PC2 
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis +1, xlab = "PC1", ylab = "PC2")  #col = 0 removing the points. Use col = 1 and col = 2, i.e. diagnosis +1)
```

Q8. Repeat the same for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[,c(1,3)], col = diagnosis +1, xlab = "PC1", ylab = "PC3") 
```

Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups. Overall, the plots indicate that principal component 1 is capturing a separation of malignant from benign samples.



#Variance explained
Scree plot
```{r}
# Variance explained by each principal component: pve
pve <- pca.var.per
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

or as a barplot
```{r}
barplot(pve, 
        names.arg=paste("PC",1:length(pca.var.per)), 
        main="Scree Plot", xlab="Principal Component", ylab="Percent Variation", 
        las=2,  #las = 2 for rotate axis labes 
        axes = FALSE)  #remove axes
axis(2, at = pve, labels = round (pca.var.per,2)*100) #play around with axis labeling
```



Using the cumsum() function, create a plot of cumulative proportion of variance explained.

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

```


Use the par() function to create a side by side plot (i.e. 1 row 2 column arrangement) of these two graphs.
```{r}
#check this out later
```


#Section 3: Hierarchical clustering of case data

note: Scoring section for each sample = Sum for(i=1, i++, (Read Count)gene_i * influence on PC1_i. do it for all PCs

Preparation for hierarchical clustering: compute distance between all pairs of observations. 

Scale the wisc.data data and assign the result to data.scaled.
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method = "complete")
plot(wisc.hclust)
abline(h=20, col="red")
```


cut into 4 clusters and check diagnosis for each one of them
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```


#Section 4: K-means clustering and comparing results

```{r}
wisc.km <- kmeans(data.scaled, centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```


Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to your hierarchical clustering model from above (wisc.hclust.clusters). Recall the cluster membership of the hierarchical clustering model is contained in wisc.hclust.clusters object.
```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```
Looking at the second table you generated, it looks like clusters 1, 2, and 4 from the hierarchical clustering model can be interpreted as the cluster 1 equivalent from the k-means algorithm, and cluster 3 can be interpreted as the cluster 2 equivalent.


#Section 5. Clustering on PCA results

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr.hclust)  #this is now much better. There are now clearly two groups
```

in table form comparing to the diagnosis
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

plot it 
```{r}
plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
```


compare the different approaches:
```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

#Section 6: sensitivity
Sensitivity refers to a test’s ability to correctly detect ill patients who do have the condition. Count of M in malignant cluster / total count of M.

Specificity relates to a test’s ability to correctly reject healthy patients without a condition. Count of B in benigh cluster / total count of B.

Q16. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
```{r}
library(rgl)

```





#Bonus content:
```{r}
##predictiong malignancy of new samples

url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <-  predict(wisc.pr, newdata=new)


```

```{r}
plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
points(npc[,1], npc[,2], col=c("purple", "blue"), pch=16, cex = 3)
```
























