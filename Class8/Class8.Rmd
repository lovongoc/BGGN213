---
title: "class8 machine learning"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# k-means clustering example
test
Let's make up some datafor testing the 'kmeans()' function

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3)) #a vector of points from two random distributions
x <- cbind(x=tmp, y=rev(tmp)) #binding it in a matrix with two columns, one with tmp and one with the reverse of tmp
plot(x)
```

```{r}
km <- kmeans(x, centers = 2, nstart = 10)
```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?

```{r}
km$size
```

Q. What ‘component’ of your result object details
      - cluster size?
      
```{r}
km$cluster
```

      - cluster assignment/membership?
      - cluster center?

```{r}
km$centers
```


Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points

```{r}
plot(x, col = km$cluster, pch = 16) #plot the data with coloring for each cluster
points(km$centers, col = "blue", pch = 16) #points() adds points on top of the graph
```



#Hierarchical clustering:

First we need to calculate point (dis)similarity as the Euclidean distance between observations
```{r}
dist_matrix <- dist(x) #calculates all pairwise distances.
dim(dist_matrix) #doesn't work as dist_matrix is not a dataframe.
dmatrix <- as.matrix(dist_matrix)
#View( dmatrix ) #view() works with dataframe. as.matrix() forces it to be a matrix.
dim(x) #60 points
dim( as.matrix(dist_matrix) ) #60x60 pairwise distances
```

Now let's use hc() 
```{r}
hc <- hclust(dist_matrix) #calculate the hierachical clustering
plot(hc) #plot as a dendogram
abline(h=4, col="red") #add a 'cutoff' line on the dendogram.
```

Cut the tree by height...
```{r}
groups <- cutree(hc, h=6) #cut by height=4
groups
table(groups)
```

Plot it
```{r}
plot(x, col=cutree(hc, h=6))
```


... or by k
```{r}
kgroups <- cutree(hc, k=4) #cut by k=4
table(kgroups)
```

and plot it
```{r}
plot(x, col=cutree(hc, k=4))
```


Using different hierarchical clustering methods to check how consistent it is
```{r}
hc.complete <- hclust(dist_matrix, method="complete")
plot(hc.complete)
hc.average  <- hclust(dist_matrix, method="average")
hc.single   <- hclust(dist_matrix, method="single")
```



#Exercise

Step 1. Generate some example data for clustering
```{r}
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
str(x)
```

Step 2. Plot the data without clustering
```{r}
plot(x)
```

Step 3. Generate colors for known clusters (just so we can compare to hclust results)

```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

```{r}
distexample <- dist(x) #calculate distances
clustexample <- hclust(distexample)
plot(clustexample) #plot as a dendogram
```

and then plot the data with different groupings
```{r}
plot(x, col=cutree(hc, k=2))
```

The clustering is not great! The differences between the different groups are relatively small. Let's try k-means to see if it gets better with a different method.

```{r}
kmexample <- kmeans(x, centers = 2, nstart = 10)
plot(x, col= kmexample$cluster)
```

looks better!



#Principal Component Analysis (PCA)

Let's retrieve the data and plot it
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1)
mydata$wt1
plot(mydata$wt1)
plot(mydata$ko1)
plot(mydata)
```

Let's call prcomp() to do PCA. However, prcomp() expects the samples to be rows and genes to be columns so we need to first transpose the matrix with the t() function.
```{r}
pca <- prcomp(t(mydata), scale=TRUE) #
attributes(pca) #pca$x contains the PCs for drawing our first graph.
```

look at pca$x
```{r}
pca$x
```

Lets plot the new PCA data: PC1 vs PC2
```{r}
plot(pca$x[,1], pca$x[,2])
```

Calculate the variance
```{r}
 ## Variance captured per PC
pca.var <- pca$sdev^2
pca.var
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) #percent value, rounded to make it look good
pca.var.per
```

Plot the variance
```{r}
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```

Make the plot more useful
```{r}
# I want to make a vector of colors for wt and ko samples
colnames(mydata) # I want my factors to be WT and KO only, not WT1 to WT5 and KO1 to KO5
substr( colnames(mydata), 1, 2) #use substring substr() to remove the extra info.
colvec <- as.factor( substr( colnames(mydata), 1, 2) ) #make a vector for colors.
```


Use the new vector to color WT and KO with two different colors
```{r}
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16, 
     xlab=paste0("PC1 (", pca.var.per[1], "%)"), #paste() concatenates strings
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))
text(pca$x[,1], pca$x[,2], labels = colnames(mydata), pos=1) #adding text labels.
#identify(pca$x[,1], pca$x[,2], labels = colnames(mydata)) #run this then click on the points on the graph that you want to identify, press ESC and it will tell you which points those are.
```


#UK food exercise
```{r}
UK_foods <- read.csv("UK_foods.csv")
```

```{r}
pcafoods <- prcomp(UK_foods, scale=TRUE)
```

First column is the name of the rows.
```{r}
updtUKfoods <- UK_foods[,2:5]
row.names(updtUKfoods) <- UK_foods$X #alternatively, you can use x <- x[,-1] to remove column 1
plot(updtUKfoods)
```

Use PCA on the dataset
```{r}
pcafoods <- prcomp(t(updtUKfoods), scale=TRUE)
plot(pcafoods$x[,1], pcafoods$x[,2])
text(pcafoods$x[,1], pcafoods$x[,2], labels = colnames(updtUKfoods))
```

Calculate variance
```{r}
pcafoods.var <- pcafoods$sdev^2
pcafoods.var.per <- round(pcafoods.var/sum(pcafoods.var)*100, 1)
barplot(pcafoods.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```


Try building a heatmap
```{r}
heatmap(as.matrix(updtUKfoods))
```






